# ENGRI 1410, Spring 2026
# HW 1
# Carl Bernard
#
# Perceptron Visual Demo with Biological Neuron Schematic
# Run in Jupyter or Google Colab
#
# Notes:
# - Linear dataset labels are generated by a hyperplane rule: w·x + b > 0
# - We intentionally do NOT use the word "true" anywhere.
# - ReLU is intentionally omitted from the activation menu at this stage.
#
# If widgets aren't installed locally, uncomment the next line:
# !pip install ipywidgets

import numpy as np
import matplotlib.pyplot as plt
from ipywidgets import interact, FloatSlider, IntSlider, Dropdown, Checkbox

# -----------------------------
# 1) Make simple 2D datasets
# -----------------------------
def make_linear_data(n=120, seed=0, w_data=(1.0, 1.0), b_data=0.0):
    """
    Linearly separable labels generated by the rule:
        y = 1  if  w_data · x + b_data > 0
            0  otherwise
    """
    rng = np.random.default_rng(int(seed))
    X = rng.normal(0, 1, size=(n, 2))
    w_data = np.array(w_data, dtype=float)
    z = X @ w_data + float(b_data)
    y = (z > 0).astype(int)
    return X, y


def make_xor_data(n=120, seed=0):
    """
    XOR-ish labels: sign(x1) XOR sign(x2)
    """
    rng = np.random.default_rng(int(seed))
    X = rng.normal(0, 1, size=(n, 2))
    y = ((X[:, 0] > 0) ^ (X[:, 1] > 0)).astype(int)
    return X, y


# -----------------------------
# 2) Perceptron forward pass
# -----------------------------
def perceptron_forward(X, w, b, activation="step"):
    """
    Computes:
        z = X @ w + b
    Returns:
        yhat: predicted class labels (0/1)
        score: z (step) or p (sigmoid)
    """
    z = X @ w + b

    if activation == "step":
        return (z > 0).astype(int), z

    if activation == "sigmoid":
        p = 1.0 / (1.0 + np.exp(-z))
        return (p >= 0.5).astype(int), p

    raise ValueError("activation must be: 'step' or 'sigmoid'")


# -----------------------------
# 3) Biological neuron sketch
# -----------------------------
def draw_bio_neuron(ax, w1, w2, b, activation_name):
    ax.axis("off")
    ax.set_title("Biological neuron analogy", fontsize=12, pad=6)

    # Normalized diagram coordinates (0..1)
    x_in1, y_in1 = 0.15, 0.75   # input 1
    x_in2, y_in2 = 0.15, 0.25   # input 2
    x_sum,  y_sum  = 0.45, 0.50 # summation node (soma)
    x_act,  y_act  = 0.70, 0.50 # activation block
    x_out,  y_out  = 0.90, 0.50 # output arrow end

    # Input circles
    for (x, y, label) in [(x_in1, y_in1, "x₁"), (x_in2, y_in2, "x₂")]:
        ax.add_patch(plt.Circle((x, y), 0.035, color="#cccccc", ec="k"))
        ax.text(x, y + 0.08, label, ha="center", fontsize=11)

    # Soma as circle
    ax.add_patch(plt.Circle((x_sum, y_sum), 0.05, color="#e6e6e6", ec="k"))
    ax.text(x_sum, y_sum, "Σ", ha="center", va="center", fontsize=12)

    # Activation box
    ax.add_patch(plt.Rectangle((x_act - 0.06, y_act - 0.045), 0.12, 0.09,
                               color="#e6f2ff", ec="k"))
    ax.text(x_act, y_act, f"activation\n({activation_name})",
            ha="center", va="center", fontsize=9)

    # Arrows: inputs -> soma
    ax.annotate("", xy=(x_sum - 0.05, y_sum + 0.02), xytext=(x_in1 + 0.04, y_in1),
                arrowprops=dict(arrowstyle="->", lw=1.5))
    ax.annotate("", xy=(x_sum - 0.05, y_sum - 0.02), xytext=(x_in2 + 0.04, y_in2),
                arrowprops=dict(arrowstyle="->", lw=1.5))

    # Weight labels
    ax.text((x_in1 + x_sum)/2 - 0.02, (y_in1 + y_sum)/2 + 0.02, f"w₁={w1:.2f}", fontsize=9)
    ax.text((x_in2 + x_sum)/2 - 0.02, (y_in2 + y_sum)/2 - 0.04, f"w₂={w2:.2f}", fontsize=9)

    # Bias arrow into soma
    ax.annotate("", xy=(x_sum, y_sum + 0.08), xytext=(x_sum, y_sum + 0.22),
                arrowprops=dict(arrowstyle="->", lw=1.5))
    ax.text(x_sum + 0.02, y_sum + 0.16, f"bias b={b:.2f}", fontsize=9)

    # Soma -> activation
    ax.annotate("", xy=(x_act - 0.06, y_act), xytext=(x_sum + 0.05, y_sum),
                arrowprops=dict(arrowstyle="->", lw=1.5))

    # Activation -> output
    ax.annotate("", xy=(x_out, y_out), xytext=(x_act + 0.06, y_act),
                arrowprops=dict(arrowstyle="->", lw=1.5))
    ax.text((x_act + x_out)/2, y_out + 0.05, "output y", fontsize=10, ha="center")

    # Process note
    ax.text(0.5, 0.08,
            "inputs → weighted sum (Σ) → add bias → activation → output",
            fontsize=9, ha="center")


# -----------------------------
# 4) Decision boundary plot
# -----------------------------
def plot_decision_boundary(ax, X, y, w1, w2, b, activation, dataset_name,
                           show_weight=True, show_region=True):
    w = np.array([w1, w2], dtype=float)

    # Predictions
    yhat, _ = perceptron_forward(X, w, b, activation)
    correct = int((yhat == y).sum())
    total = int(len(y))
    incorrect = total - correct

    # Shaded regions (optional)
    if show_region:
        xx, yy = np.meshgrid(np.linspace(-3, 3, 301), np.linspace(-3, 3, 301))
        grid = np.c_[xx.ravel(), yy.ravel()]
        grid_pred, _ = perceptron_forward(grid, w, b, activation)
        ax.contourf(
            xx, yy, grid_pred.reshape(xx.shape),
            levels=[-1, 0, 1],
            alpha=0.15,
            colors=["#4a90e2", "#e24a4a"]
        )

    # Data points
    ax.scatter(X[:, 0], X[:, 1], c=y, cmap="bwr", edgecolor="k", s=45)

    # Decision boundary: w·x + b = 0
    if np.linalg.norm(w) > 1e-6:
        if abs(w[1]) > 1e-6:
            xs = np.linspace(-3, 3, 200)
            ys = -(w[0]/w[1]) * xs - b / w[1]
            ax.plot(xs, ys, "k-", lw=2)
        else:
            x0 = -b / w[0]
            ax.plot([x0, x0], [-3, 3], "k-", lw=2)

    # Weight vector arrow (optional)
    if show_weight and np.linalg.norm(w) > 1e-6:
        w_display = (w / np.linalg.norm(w)) * 1.8
        ax.arrow(0.0, 0.0, w_display[0], w_display[1],
                 head_width=0.12, head_length=0.18,
                 fc="k", ec="k", length_includes_head=True)
        ax.text(w_display[0]*1.05, w_display[1]*1.05, "w", fontsize=12)

    ax.set_xlim(-3, 3)
    ax.set_ylim(-3, 3)
    ax.set_xlabel("x₁")
    ax.set_ylabel("x₂")

    title = (f"{dataset_name}  |  activation: {activation}\n"
             f"w = [{w1:.2f}, {w2:.2f}],  b = {b:.2f}   |   "
             f"correct: {correct}/{total}, misclassified: {incorrect}")
    ax.set_title(title, fontsize=11)
    ax.grid(alpha=0.25)


# -----------------------------
# 5) Widget-driven demo
# -----------------------------
def demo(activation="step", dataset="linear", w1=1.0, w2=-1.0, b=0.0,
         show_weight=True, show_region=True, seed=0):

    if dataset == "linear":
        X, y = make_linear_data(seed=seed, w_data=(1.0, 1.0), b_data=0.0)
        dataset_name = r"Linearly separable data ($w \cdot x + b > 0$)"
    else:
        X, y = make_xor_data(seed=seed)
        dataset_name = "XOR-style data"

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 5.8))
    plt.subplots_adjust(wspace=0.3)

    plot_decision_boundary(ax1, X, y, w1, w2, b, activation, dataset_name,
                           show_weight=show_weight, show_region=show_region)
    draw_bio_neuron(ax2, w1, w2, b, activation)

    plt.show()


# Interactive UI (ReLU intentionally omitted)
interact(
    demo,
    activation=Dropdown(options=["step", "sigmoid"], value="step", description="activation"),
    dataset=Dropdown(options=["linear", "xor"], value="linear", description="dataset"),
    w1=FloatSlider(min=-3, max=3, step=0.1, value=1.0, description="w₁"),
    w2=FloatSlider(min=-3, max=3, step=0.1, value=-1.0, description="w₂"),
    b=FloatSlider(min=-3, max=3, step=0.1, value=0.0, description="bias b"),
    show_weight=Checkbox(value=True, description="show weight vector"),
    show_region=Checkbox(value=True, description="shade regions"),
    seed=IntSlider(min=0, max=9, step=1, value=0, description="data seed")
);
